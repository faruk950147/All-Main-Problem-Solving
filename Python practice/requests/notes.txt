requests হলো Python-এর একটি জনপ্রিয় লাইব্রেরি, যেটি দিয়ে আপনি সহজে HTTP রিকোয়েস্ট (যেমন GET, POST) পাঠাতে পারেন — ওয়েবসাইট থেকে ডেটা আনার জন্য।

requests দিয়ে কী করা যায়?
কাজ	উদাহরণ
ওয়েব পেজ লোড করা	ওয়েবসাইটের HTML কনটেন্ট আনা
API কল করা	JSON ডেটা পাওয়া
ফর্ম ডেটা পাঠানো	POST রিকোয়েস্ট দিয়ে লগইন বা সাবমিট
ফাইল ডাউনলোড	ছবি, পিডিএফ ডাউনলোড করা

ইনস্টল:
bash
Copy
Edit
pip install requests
সাধারণ উদাহরণ (GET রিকোয়েস্ট):
python
Copy
Edit
import requests

url = "https://example.com"
response = requests.get(url)

print(response.status_code)  # 200 হলে success
print(response.text)         # HTML কনটেন্ট
POST রিকোয়েস্ট (ফর্ম সাবমিট):
python
Copy
Edit
import requests

url = "https://httpbin.org/post"
data = {
    "username": "omar",
    "password": "1234"
}

response = requests.post(url, data=data)

print(response.text)
JSON API থেকে ডেটা আনা:
python
Copy
Edit
import requests

url = "https://jsonplaceholder.typicode.com/todos/1"
response = requests.get( url)

data = response.json()
print(data['title'])
ফাইল ডাউনলোড (ছবি বা PDF):
python
Copy
Edit
import requests

url = "https://example.com/image.jpg"
response = requests.get(url)

with open("image.jpg", "wb") as f:
    f.write(response.content)

try:
    response = requests.get("https://invalid-url.com")
    response.raise_for_status()  # Error থাকলে exception ছুঁড়ে দিবে
except requests.exceptions.RequestException as e:
    print("Error:", e)


========================= Web Scraping =========================
web scraping হলো ওয়েবসাইট থেকে নির্দিষ্ট তথ্য টেনে আনা। 
Web Scraping-এ requests ব্যবহার করে:
python
Copy
Edit
import requests
from bs4 import BeautifulSoup

url = "https://example.com"
response = requests.get(url)
soup = BeautifulSoup(response.text, 'html.parser')

for p in soup.find_all('p'):
    print(p.text)


========================= শেষ কথা =========================
requests হলো Web Scraping বা API কাজ করার সবচেয়ে সহজ এবং শক্তিশালী টুলগুলোর একটি।

আপনি যদি Daraz/Wikipedia বা অন্য কোনো সাইট থেকে ডেটা আনতে চান — শুধু বলুন, আমি আপনার জন্য ফুল কোড লিখে দিচ্ছি বাংলায়।


=========================== Web Crawling =========================

Web crawling হলো একটি ওয়েবসাইটের প্রতিটি পৃষ্ঠা (page) স্বয়ংক্রিয়ভাবে ভিজিট বা পাঠানো করার প্রক্রিয়া। 
এই প্রক্রিয়ায় একটি প্রোগ্রাম (যাকে Crawler বা Spider বলা হয়)
নির্দিষ্ট একটি ওয়েবসাইটের লিংক ধরে ধরে সকল পাতায় যায় এবং সেখান থেকে তথ্য সংগ্রহ করে।

এটি সাধারণত সার্চ ইঞ্জিন (যেমন Google, Bing) ব্যবহার করে থাকে একটি সাইটের সকল পৃষ্ঠা স্ক্যান করার জন্য, 
যেন তারা তাদের ডাটাবেজে সংরক্ষণ করতে পারে এবং ব্যবহারকারীদের সার্চ ফলাফলে দেখাতে পারে।


=========================== শেষ কথা =========================
requests ব্যবহার করে একটি সাইটের সব পাতা পাঠানো করা:
python
Copy
Edit
import requests
from bs4 import BeautifulSoup

url = "https://example.com"
response = requests.get(url)
soup = BeautifulSoup(response.text, 'html.parser')

for link in soup.find_all('a'):
    print(link.get('href'))